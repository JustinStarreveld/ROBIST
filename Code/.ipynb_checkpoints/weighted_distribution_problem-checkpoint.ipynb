{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b1277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external packages\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "# import mosek\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# import internal packages\n",
    "import phi_divergence as phi\n",
    "from iter_gen_and_eval_alg import iter_gen_and_eval_alg\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496bc27",
   "metadata": {},
   "source": [
    "The problem we examine is as follows:\n",
    "\n",
    "Consider a company that sells $n$ different products, which it is able to produce with the use of $m$ different machines. \n",
    "\n",
    "The goal is to determine an optimal production plan, which specifies the amount of time $y_{jk}$ that each machine $j=1,\\dots,m$ will be used for producing product $k = 1,\\dotsc,n$. An optimal plan is one that maximizes the total profit of the company subject to availability constraints.\n",
    "\n",
    "Each machine $j$ may only be used for a limited amount of time $a_j$ and incurs operating costs $c_{jk}$ per unit of product $k$ that is produced. Each unit of product $k$ can be sold at a price of $u_k$ and leftover units incur inventory holding costs of $\\tilde{c}_{k}$. For this problem there are two uncertain parameters, the demand $d_k$ for each product $k$ and the quantity $p_{jk}$ of product $k$ that is produced per time unit by machine $j$. \n",
    "\n",
    "Putting all this together, we arrive at the following mathematical formulation:\n",
    "\\begin{align}\n",
    "    \\label{mathform:weighted_dist_1:obj}\n",
    "    \\min_{\\mathbf{y}}&~\\sum_{j=1}^{m} \\sum_{k=1}^{n} c_{jk} y_{jk} + \\sum_{k=1}^{n} \\tilde{c}_{k} \\left[ \\sum_{j=1}^{m} p_{jk} y_{jk} - d_k \\right]_{+} - \\sum_{k=1}^{n} u_k \\min \\left( \\sum_{j=1}^{m} p_{jk} y_{jk}, d_k \\right) \\\\\n",
    "    \\text{s.t.}&~\\sum_{k=1}^{n} y_{jk} \\leq a_j, && j=1.\\dotsc,m \\label{mathform:weighted_dist_1:limit_time}\\\\\n",
    "    &~y_{jk} \\geq 0, && j=1.\\dotsc,m, k=1.\\dotsc,n, \\label{mathform:weighted_dist_1:xVarNonNeg}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $[ a ]_{+}$ is equivalent to $\\max\\{a,0\\}$. The $\\max$ and $\\min$ functions make this a nonlinear optimization problem. As such, it is quite a tricky problem to deal with using conventional Robust Optimization techniques. \n",
    "\n",
    "Note that this problem can also be easily converted into the notation used earlier for by moving the uncertainty into the constraints and introducing supplementary variable $\\theta$.\n",
    "Let $\\mathbf{x} = (\\mathbf{y}, \\theta)$ and $\\mathscr{X} = \\{ \\mathbf{x} : \\sum_{k=1}^{n} y_{jk} \\leq a_j, j=1.\\dotsc,m, \\mathbf{y} \\geq 0 \\}$. We can rewrite the problem as:\n",
    "\\begin{align}\n",
    "    \\label{mathform:weighted_dist_2:obj}\n",
    "    \\min_{\\mathbf{x} \\in \\mathscr{X}}&~\\sum_{j=1}^{m} \\sum_{k=1}^{n} c_{jk} y_{jk} + \\theta  \\\\\n",
    "    \\text{s.t.}&~\\sum_{k=1}^{n} \\tilde{c}_{k} \\left[ \\sum_{j=1}^{m} p_{jk} y_{jk} - d_k \\right]_{+} - \\sum_{k=1}^{n} u_k \\min \\left( \\sum_{j=1}^{m} p_{jk} y_{jk}, d_k \\right) - \\theta \\leq 0.\n",
    "\\end{align}\n",
    "\n",
    "By defining $g(\\mathbf{x}) = \\sum_{j=1}^{m} \\sum_{k=1}^{n} c_{jk} y_{jk} + \\theta$ and $f(\\mathbf{x}, \\mathbf{z}) = \\sum_{k=1}^{n} \\tilde{c}_{k} \\left[ \\sum_{j=1}^{m} p_{jk} y_{jk} - d_k \\right]_{+} - \\sum_{k=1}^{n} u_k \\min \\left( \\sum_{j=1}^{m} p_{jk} y_{jk}, d_k \\right) - \\theta$ we obtain the familiar notation used throughout this paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21db292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem specific functions:\n",
    "def generate_unc_param_data(random_seed, N, **kwargs):\n",
    "    np.random.seed(random_seed)\n",
    "    scale_dim_problem = kwargs.get('scale_dim_problem', 1)\n",
    "    m = 5*scale_dim_problem\n",
    "    n = 10*scale_dim_problem\n",
    "    \n",
    "    # generate demand vector param\n",
    "    d_care = np.array([25, 38, 18, 39, 60, 35, 41, 22, 74, 30])\n",
    "    d_nom = ()\n",
    "    for i in range(scale_dim_problem):\n",
    "        d_i = d_care\n",
    "        d_nom = d_nom + tuple(d_i.reshape(1, -1)[0])\n",
    "    d = np.random.default_rng(seed=random_seed).dirichlet(d_nom, N) * sum(d_nom)\n",
    "    \n",
    "    # generate production efficiency param\n",
    "    p_care = np.array([[5.0, 7.6, 3.6, 7.8, 12.0, 7.0, 8.2, 4.4, 14.8, 6.0],\n",
    "                      [3.8, 5.8, 2.8, 6.0, 9.2, 5.4, 6.3, 3.4, 11.4, 4.6],\n",
    "                      [2.3, 3.5, 1.6, 3.5, 5.5, 3.2, 3.7, 2.0, 6.7, 2.7],\n",
    "                      [2.6, 4.0, 1.9, 4.1, 6.3, 3.7, 4.3, 2.3, 7.8, 3.2],\n",
    "                      [2.4, 3.6, 1.7, 3.7, 5.7, 3.3, 3.9, 2.1, 7.0, 2.9]])\n",
    "    if scale_dim_problem > 1:\n",
    "        p_nom = np.block([[p_care for i in range(scale_dim_problem)] for j in range(scale_dim_problem)])\n",
    "    else:\n",
    "        p_nom = p_care\n",
    "    p = np.random.random_sample(size = (N,m,n)) * (p_nom*1.05 - p_nom*0.95) + (p_nom*0.95)\n",
    "    data = list(zip(d,p))\n",
    "    data = np.array(data, dtype=object)\n",
    "    return data\n",
    "\n",
    "def get_fixed_param_data(random_seed, **kwargs):\n",
    "    np.random.seed(random_seed)\n",
    "    scale_dim_problem = kwargs.get('scale_dim_problem', 1)\n",
    "    \n",
    "    # fixed parameter values from Care (2014)\n",
    "    C_care = np.array([[1.8, 2.2, 1.5, 2.2, 2.6, 2.1, 2.2, 1.7, 2.8, 1.9],\n",
    "                        [1.6, 1.9, 1.3, 1.9, 2.3, 1.9, 2.0, 1.5, 2.5, 1.7],\n",
    "                        [1.2, 1.5, 1.0, 1.5, 1.9, 1.4, 1.6, 1.1, 2.0, 1.3],\n",
    "                        [1.3, 1.6, 1.1, 1.6, 2.0, 1.5, 1.7, 1.2, 2.2, 1.4],\n",
    "                        [1.2, 1.5, 1.0, 1.6, 1.9, 1.5, 1.6, 1.1, 2.1, 1.3]])\n",
    "\n",
    "    A_care = np.array([10, 13, 22, 19, 21])\n",
    "    C_tilde_care = np.array([1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3])\n",
    "    U_care = np.array([1.5, 1.8, 1.2, 1.9, 2.2, 1.8, 1.9, 1.4, 2.4, 1.6])\n",
    "    \n",
    "    if scale_dim_problem > 1:\n",
    "        max_deviation = 0.10 # represents max deviation from care values\n",
    "        C = np.block([[np.random.random_sample(size = (5,10)) * (C_care*(1+max_deviation) - C_care*(1-max_deviation)) + (C_care*(1-max_deviation)) for i in range(scale_dim_problem)] \n",
    "                      for j in range(scale_dim_problem)])\n",
    "        \n",
    "        A = np.round(np.block([np.random.random_sample(size = 5) * (A_care*(1+max_deviation) - A_care*(1-max_deviation)) + (A_care*(1-max_deviation)) for i in range(scale_dim_problem)]))\n",
    "        \n",
    "        C_tilde = np.block([np.random.random_sample(size = 10) * (C_tilde_care*(1+max_deviation) - C_tilde_care*(1-max_deviation)) + (C_tilde_care*(1-max_deviation)) for i in range(scale_dim_problem)])\n",
    "        \n",
    "        U = np.block([np.random.random_sample(size = 10) * (U_care*(1+max_deviation) - U_care*(1-max_deviation)) + (U_care*(1-max_deviation)) for i in range(scale_dim_problem)])\n",
    "        param_dict = {'C':C, 'A':A, 'C_tilde': C_tilde, 'U': U}\n",
    "    else:\n",
    "        param_dict = {'C':C_care, 'A':A_care, 'C_tilde': C_tilde_care, 'U': U_care}\n",
    "    \n",
    "    return param_dict\n",
    "\n",
    "def solve_P_SCP(S, **kwargs):\n",
    "    # get fixed parameter values\n",
    "    C = kwargs['C']\n",
    "    A = kwargs['A']\n",
    "    C_tilde = kwargs['C_tilde']\n",
    "    U = kwargs['U']\n",
    "    \n",
    "    # unzip uncertain parameters\n",
    "    d,p = data.T\n",
    "    \n",
    "    # get dimensions of problem\n",
    "    m,n = p[0].shape\n",
    "    num_scen = len(d)\n",
    "    \n",
    "    # create variables\n",
    "    theta = cp.Variable(1)\n",
    "    y = cp.Variable((m, n), nonneg = True)\n",
    "    \n",
    "    # set up problem\n",
    "    setup_time_start = time.time()\n",
    "    constraints = []\n",
    "    for s in range(num_scen):\n",
    "        prod_s = cp.sum(cp.multiply(p[s], y), axis=0)\n",
    "        unc_inv_cost_s = C_tilde.T @ cp.pos(prod_s - d[s])\n",
    "        unc_rev_s = U.T @ cp.minimum(prod_s, d[s])\n",
    "\n",
    "        constraints.append(unc_inv_cost_s - unc_rev_s - theta <= 0)\n",
    "    \n",
    "    constraints.append(cp.sum(y, axis=1) <= A)\n",
    "    \n",
    "    fixed_costs = cp.sum(cp.multiply(C, y))\n",
    "    obj = cp.Minimize(fixed_costs + theta)\n",
    "    prob = cp.Problem(obj,constraints)\n",
    "    \n",
    "    # solve problem\n",
    "    time_limit = kwargs.get('time_limit', 2*60*60) - (time.time() - setup_time_start)\n",
    "    if time_limit < 0:\n",
    "        print(\"Error: did not provide sufficient time for setting up & solving problem\")\n",
    "        return (None, None)\n",
    "    \n",
    "#     prob.solve(solver=cp.MOSEK, mosek_params = {mosek.dparam.optimizer_max_time: time_limit})\n",
    "    prob.solve(solver=cp.GUROBI, verbose=False, Threads=1, TimeLimit=time_limit)\n",
    "    x_value = [theta.value, y.value] # Combine y and theta into 1 single solution vector\n",
    "    return (x_value, prob.value)\n",
    "\n",
    "def unc_obj_func(x, data, **kwargs):\n",
    "    # extract values\n",
    "    C = kwargs['C']\n",
    "    C_tilde = kwargs['C_tilde']\n",
    "    U = kwargs['U']\n",
    "    d,p = data.T\n",
    "    m,n = p[0].shape\n",
    "    y = x[1]\n",
    "    \n",
    "    # compute obj function value:\n",
    "    fixed_cost = np.sum(np.multiply(C, y))\n",
    "    prod = [np.einsum('jk,jk->k', p[s], y) for s in range(len(data))]\n",
    "    inventory_cost = np.array([np.dot(C_tilde, np.maximum(prod[s] - d[s],0)) for s in range(len(data))]) \n",
    "    revenue = np.array([np.dot(U, np.minimum(prod[s], d[s])) for s in range(len(data))]) \n",
    "    \n",
    "    return fixed_cost + inventory_cost - revenue\n",
    "\n",
    "def eval_x_OoS(x, obj, data, eval_unc_obj, **kwargs):\n",
    "    unc_obj_func = eval_unc_obj['function']\n",
    "    desired_rhs = eval_unc_obj['info']['desired_rhs']\n",
    "    \n",
    "    evals = unc_obj_func(x, data, **kwargs)  \n",
    "    p_vio = sum(evals>(obj+(1e-6))) / len(data) \n",
    "    VaR = - np.quantile(evals, desired_rhs, method='inverted_cdf')\n",
    "    return p_vio, VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6098d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "TIME_LIMIT = 1*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6340abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide functions and other info for generating & evaluating solutions\n",
    "solve_SCP = solve_P_SCP\n",
    "scale_dim_problem = 3\n",
    "problem_instance = get_fixed_param_data(random_seed, scale_dim_problem=scale_dim_problem)\n",
    "problem_instance['time_limit'] = TIME_LIMIT \n",
    "\n",
    "eval_unc_obj = {'function': unc_obj_func,\n",
    "                'info': {'risk_measure': 'probability', # must be either 'probability' or 'expectation'\n",
    "                         'desired_rhs': 1 - 0.01}}\n",
    "\n",
    "eval_unc_constr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate extra out-of-sample (OoS) data\n",
    "random_seed = 1234\n",
    "N_OoS = int(1e5)\n",
    "data_OoS = generate_unc_param_data(1234, N_OoS, scale_dim_problem=scale_dim_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77480a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classic approach:\n",
    "random_seed = 0\n",
    "N = 100 #10580\n",
    "data = generate_unc_param_data(random_seed, N, scale_dim_problem=scale_dim_problem)\n",
    "\n",
    "start_time = time.time()\n",
    "x, obj = solve_P_SCP(data, **problem_instance)\n",
    "runtime_classic = time.time() - start_time\n",
    "obj_classic = - obj\n",
    "# p_vio_classic, VaR_classic = eval_x_OoS(x, obj, data_OoS, eval_unc_obj['info']['desired_rhs'], **problem_instance)\n",
    "\n",
    "print(N, runtime_classic, obj_classic, p_vio_classic, VaR_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8651a7d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (11073,15,30) (10,30) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20344\\759060988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m x_care, obj_care, N_2, runtime = util.solve_with_care2014(solve_SCP, problem_instance, generate_unc_param_data, \n\u001b[0m\u001b[0;32m      9\u001b[0m                                                     \u001b[0meval_unc_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_param_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                                     random_seed=random_seed, scale_dim_problem=scale_dim_problem)\n",
      "\u001b[1;32m~\\OneDrive - UvA\\Research\\Paper with Guan\\Code\\SamplingRobust\\Code\\util.py\u001b[0m in \u001b[0;36msolve_with_care2014\u001b[1;34m(solve_SCP, problem_instance, generate_unc_param_data, eval_unc_obj, conf_param_alpha, dim_x, N_1, random_seed, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# (2) sample N_1 and N_2 independent scenarios\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_unc_param_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mN_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m     \u001b[0mdata_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mN_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20344\\2965471899.py\u001b[0m in \u001b[0;36mgenerate_unc_param_data\u001b[1;34m(random_seed, N, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mp_nom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_care\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp_nom\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1.05\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mp_nom\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp_nom\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (11073,15,30) (10,30) "
     ]
    }
   ],
   "source": [
    "# Care (2014) approach:\n",
    "generate_unc_param_data = generate_unc_param_data\n",
    "conf_param_alpha = 1e-9\n",
    "dim_x = 5*scale_dim_problem * 10*scale_dim_problem\n",
    "N_1 = 20 * dim_x\n",
    "random_seed = 0\n",
    "\n",
    "x_care, obj_care, N_2, runtime = util.solve_with_care2014(solve_SCP, problem_instance, generate_unc_param_data, \n",
    "                                                    eval_unc_obj, conf_param_alpha, dim_x, N_1=N_1,\n",
    "                                                    random_seed=random_seed, scale_dim_problem=scale_dim_problem)\n",
    "\n",
    "p_vio_care, VaR_care = eval_x_OoS(x, obj, data_OoS, eval_unc_obj['info']['desired_rhs'], **problem_instance)\n",
    "\n",
    "print(N_1, N_2, runtime, obj_care, p_vio_care, VaR_care)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a96f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and split data into train and test\n",
    "random_seed = 0\n",
    "N_total = 10000\n",
    "data = generate_unc_param_data(random_seed, N_total)\n",
    "\n",
    "N_train = N_total / 2\n",
    "data_train, data_test = train_test_split(data, train_size=(N_train/N_total), random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7588bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our own algorithm parameter values\n",
    "conf_param_alpha = 1e-9\n",
    "add_strategy = 'random_vio'\n",
    "remove_strategy = 'random_any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43062d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the algorithm\n",
    "alg = iter_gen_and_eval_alg(solve_SCP, problem_instance, eval_unc_obj, eval_unc_constr, \n",
    "                            data_train, data_test, conf_param_alpha=conf_param_alpha,\n",
    "                            verbose=False)\n",
    "\n",
    "\n",
    "stop_criteria={'max_elapsed_time': TIME_LIMIT} # in seconds (time provided to search algorithm)\n",
    "\n",
    "(best_sol, runtime, num_iter, pareto_frontier, S_history) = alg.run(stop_criteria=stop_criteria)\n",
    "print(best_sol, runtime, num_iter, pareto_frontier, S_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e5c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d0fa288",
   "metadata": {},
   "source": [
    "# Now for the computational experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed3510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1265819",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = 'wdp_care2014_scale=1_eps=0.10_seeds=1-10'\n",
    "\n",
    "headers = ['dim_x', 'seed', \n",
    "           '$N^{Classic}$', '$T^{Classic}$', '$Obj.~(Classic)$', '$p_{vio}^{OoS}~(Classic)$', '$VaR^{OoS}~(Classic)$',\n",
    "           '$N_1$', '$N_2$', '$T$', '$Obj.$', '$p_{vio}^{OoS}$', '$VaR^{OoS}$',\n",
    "           '$N_1$', '$N_2$', '$T$', '$Obj.$', '$p_{vio}^{OoS}$', '$VaR^{OoS}$',\n",
    "           '\\#Iter.~(\\\\texttt{add})', '\\#Iter.~(\\\\texttt{remove})', \n",
    "           '$\\mu_{|\\mathcal{S}_i|}$', '$\\max_{i}|\\mathcal{S}_i|$']\n",
    "\n",
    "# Write headers to .txt file\n",
    "with open(r'output/WeightedDistributionProblem/headers_'+output_file_name+'.txt','w+') as f:\n",
    "    f.write(str(headers))\n",
    "\n",
    "output_data = {}\n",
    "\n",
    "random_seed_settings = [i for i in range(1, 11)]\n",
    "\n",
    "# fixed info:\n",
    "solve_SCP = solve_P_SCP\n",
    "\n",
    "eval_unc_obj = {'function': unc_obj_func,\n",
    "                    'info': {'risk_measure': 'probability'}}\n",
    "eval_unc_constr = None\n",
    "conf_param_alpha = 1e-9\n",
    "scale_dim_problem = 1\n",
    "dim_x = 5*scale_dim_problem * 10*scale_dim_problem\n",
    "\n",
    "random_seed = 1234\n",
    "N_OoS = int(1e5)\n",
    "data_OoS = generate_unc_param_data(1234, N_OoS, scale_dim_problem=scale_dim_problem)\n",
    "\n",
    "epsilon = 0.10\n",
    "eval_unc_obj['info']['desired_rhs'] = 1 - epsilon\n",
    "\n",
    "run_count = 0\n",
    "for random_seed in random_seed_settings:\n",
    "\n",
    "    problem_instance = get_fixed_param_data(random_seed, scale_dim_problem=scale_dim_problem)\n",
    "    problem_instance['time_limit'] = 1*60*60 # maximum on SCP runtime (in seconds) \n",
    "\n",
    "    # classic approach:\n",
    "#     N_classic = 34918 # for scale=2\n",
    "    N_classic = util.determine_campi_N_min(dim_x, 1-epsilon, conf_param_alpha)\n",
    "    data = generate_unc_param_data(random_seed, N_classic)\n",
    "    start_time = time.time()\n",
    "    x, obj = solve_P_SCP(data, **problem_instance)\n",
    "    runtime_classic = time.time() - start_time\n",
    "    obj_classic = - obj\n",
    "    p_vio_classic, VaR_classic = eval_x_OoS(x, obj, data_OoS, eval_unc_obj['info']['desired_rhs'], **problem_instance)\n",
    "    \n",
    "    # FAST approach\n",
    "    N_1 = 20 * dim_x # using the rule of thumb proposed in their paper\n",
    "    x, obj, N_2, runtime = util.solve_with_care2014(solve_SCP, problem_instance, generate_unc_param_data, \n",
    "                                                    eval_unc_obj, conf_param_alpha, dim_x, N_1=N_1,\n",
    "                                                    random_seed=random_seed,\n",
    "                                                    scale_dim_problem=scale_dim_problem)\n",
    "    runtime_FAST = runtime \n",
    "    obj_FAST = - obj\n",
    "    p_vio_FAST, VaR_FAST = eval_x_OoS(x, obj, data_OoS, eval_unc_obj['info']['desired_rhs'], **problem_instance)\n",
    "    \n",
    "    # Our method\n",
    "    # N_total = N_classic\n",
    "    N_total = N_1 + N_2\n",
    "    data = generate_unc_param_data(random_seed, N_total, scale_dim_problem=scale_dim_problem)\n",
    "    N_train = N_total / 2\n",
    "    data_train, data_test = train_test_split(data, train_size=(N_train/N_total), random_state=random_seed)\n",
    "    \n",
    "    alg = iter_gen_and_eval_alg(solve_SCP, problem_instance, eval_unc_obj, eval_unc_constr, \n",
    "                                data_train, data_test, conf_param_alpha=conf_param_alpha,\n",
    "                                verbose=False)\n",
    "    \n",
    "    if 'N2_min' not in eval_unc_obj['info'].keys():\n",
    "        N2 = N_total / 2\n",
    "        desired_rhs = 1 - epsilon\n",
    "        N2_min = alg._determine_N_min(N2, desired_rhs)\n",
    "        eval_unc_obj['info']['N2_min'] = N2_min\n",
    "    \n",
    "    stop_criteria={'max_elapsed_time': runtime_FAST} # in seconds (time provided to search algorithm)\n",
    "\n",
    "    (best_sol, runtime, num_iter, pareto_frontier, S_history) = alg.run(stop_criteria=stop_criteria)\n",
    "    \n",
    "    obj_ISSwRA = - best_sol['obj']\n",
    "    p_vio_ISSwRA, VaR_ISSwRA = eval_x_OoS(best_sol['sol'], best_sol['obj'], data_OoS, eval_unc_obj['info']['desired_rhs'], **problem_instance)\n",
    "    S_avg = sum(len(S_i) for S_i in S_history) / len(S_history)\n",
    "    S_max = max(len(S_i) for S_i in S_history)\n",
    "    \n",
    "    output_data[(dim_x, random_seed)] = [N_classic, runtime_classic, obj_classic, p_vio_classic, VaR_classic,\n",
    "                                        #np.nan, np.nan, np.nan, np.nan, np.nan,\n",
    "                                        N_1, N_2, runtime_FAST, obj_FAST, p_vio_FAST, VaR_FAST,\n",
    "                                        N_train, (N_total-N_train), runtime, obj_ISSwRA, p_vio_ISSwRA, VaR_ISSwRA,\n",
    "                                        num_iter['add'], num_iter['remove'], S_avg, S_max]\n",
    "\n",
    "    output_file_name = 'new_output_data'\n",
    "    with open(r'output/WeightedDistributionProblem/'+output_file_name+'.txt','w+') as f:\n",
    "        f.write(str(output_data))\n",
    "\n",
    "    run_count += 1\n",
    "    print(\"Completed run: \" + str(run_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1511ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "\n",
    "output_file_name = 'wdp_care2014_scale=1_eps=0.01_seeds=1-10'\n",
    "# Read from .txt file\n",
    "file_path = 'output/WeightedDistributionProblem/'+output_file_name+'.txt'\n",
    "dic = ''\n",
    "with open(file_path,'r') as f:\n",
    "     for i in f.readlines():\n",
    "        if i != \"nan\":\n",
    "            dic=i #string\n",
    "output_data_read = eval(dic)\n",
    "output_data_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf6314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_data = output_data_read\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce563bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain average and std dev\n",
    "import pandas as pd\n",
    "df_output = pd.DataFrame.from_dict(output_data, orient='index')\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_str = {}\n",
    "for i,res in output_data.items():\n",
    "    res_str = []\n",
    "    for i2,el in enumerate(res):\n",
    "        i3 = i2-5\n",
    "        if np.isnan(el):\n",
    "            res_str.append('-')\n",
    "                \n",
    "        elif i3 in [0,1,6,7]:\n",
    "            res_str.append(f'{round(df_output.iloc[:,i2].mean(),0):.0f}') \n",
    "        elif i3 in [2,8]:\n",
    "            res_str.append(f'{round(df_output.iloc[:,i2].mean(),0):.0f}' + \" (\"+f'{round(df_output.iloc[:,i2].std(),0):.0f}'+\")\")\n",
    "        elif i3 in [12,13,14,15]:\n",
    "            res_str.append(f'{round(df_output.iloc[:,i2].mean(),1):.1f}' + \" (\"+f'{round(df_output.iloc[:,i2].std(),1):.1f}'+\")\")\n",
    "        elif i3 in [3,5,9,11]:\n",
    "            res_str.append(f'{round(df_output.iloc[:,i2].mean(),2):.2f}' + \" (\"+f'{round(df_output.iloc[:,i2].std(),2):.2f}'+\")\")\n",
    "        else:\n",
    "            res_str.append(f'{round(df_output.iloc[:,i2].mean(),4):.4f}' + \" (\"+f'{round(df_output.iloc[:,i2].std(),4):.4f}'+\")\")\n",
    "    \n",
    "    output_data_str[i] = res_str\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d55ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0af738",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['dim_x', 'seed', \n",
    "           '$N^{Classic}$', '$T^{Classic}$', '$Obj.~(Classic)$', '$p_{vio}^{OoS}~(Classic)$', '$VaR^{OoS}~(Classic)$',\n",
    "           '$N_1$', '$N_2$', '$T$', '$Obj.$', '$p_{vio}^{OoS}$', '$VaR^{OoS}$',\n",
    "           '$N_1$', '$N_2$', '$T$', '$Obj.$', '$p_{vio}^{OoS}$', '$VaR^{OoS}$',\n",
    "           '\\#Iter.~(\\\\texttt{add})', '\\#Iter.~(\\\\texttt{remove})', \n",
    "           '$\\mu_{|\\mathcal{S}_i|}$', '$\\max_{i}|\\mathcal{S}_i|$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataio\n",
    "dataio.write_output_to_latex(2, headers, output_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c42298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
